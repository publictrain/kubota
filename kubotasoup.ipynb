from bs4 import BeautifulSoup
import re
import requests
import time
import random
import csv
import urllib
import os
import pprint
import time
import urllib.error
import urllib.request


headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36"}

def main():
  ans_list = []
  ans_list_box = []
  pdf_list = []
  k=int(input("(ここに入力した数の安保理データが手に入るよ！"))

  for i in range(1,k):#1ページ目からスタート
    load_url = "https://www.securitycouncilreport.org/un-documents-all-pdfs-last-added/page/{0}".format(i)# これヘッダーがないと動かない。スクレイピングよけだろう...
    html = requests.get(load_url,headers=headers)
    soup = BeautifulSoup(html.content, "html.parser")
    elems = soup.select("h7 > a")# elemsの型はBs特有のもの、h7 > aでネストの探索

    for elem in elems:
      if "pdf" in str(elem):
        pdf_list.append(str(elem.get("href")))#　elemの型はBs特有のもの、それをキャストしてる
    wait_xs()
  print(pdf_list)

  for j in range(len(pdf_list)):
    req = urllib.request.Request(pdf_list[j], headers=headers)# 403よけHeader
    download_file(req, "/content/drive/MyDrive/Colab Notebooks/kubota/gomi/pekopeko{0}.pdf".format(j))


def wait_xs():
  sec = random.uniform(0.1, 0.5)
  time.sleep(sec)

def download_file(req, dst_path):
    try:
        with urllib.request.urlopen(req) as web_file:
            data = web_file.read()
            with open(dst_path, mode='wb') as local_file:
                local_file.write(data)
    except urllib.error.URLError as e:
        print(e)



if __name__ == "__main__":
    main()


